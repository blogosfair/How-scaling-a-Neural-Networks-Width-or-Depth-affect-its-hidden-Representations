<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>How scaling a neural networks width or depth affect its hidden representations | ICLR Blogposts 2023 (staging)</title> <meta name="author" content="abc b c"/> <meta name="description" content="This blog post provides an interactive journey through a representation analysis in deep and wide neural networks."/> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations/assets/img/iclr_favicon.ico"/> <link rel="stylesheet" href="/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations/assets/css/main.css"> <link rel="canonical" href="https://blogosfair.github.io/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations/blog/2022/how-scaling-a-neural-networks-width-or-depth-affect-its-hidden-representations/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations/assets/js/theme.js"></script> <script src="/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations/assets/js/distillpub/template.v2.js"></script> <script src="/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations/assets/js/distillpub/transforms.v2.js"></script> <script src="/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations/assets/js/distillpub/overrides.js"></script> </head> <d-front-matter> <script async type="text/json">{
      "title": "How scaling a neural networks width or depth affect its hidden representations",
      "description": "This blog post provides an interactive journey through a representation analysis in deep and wide neural networks.",
      "published": "December 1, 2022",
      "authors": [
        {
          "author": "Anonymous",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations//">ICLR Blogposts 2023 (staging)</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations/about">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations/call">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations/submitting">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" target="_blank" rel="noopener noreferrer">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>How scaling a neural networks width or depth affect its hidden representations</h1> <p>This blog post provides an interactive journey through a representation analysis in deep and wide neural networks.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#centered-kernel-alignment">Centered kernel alignment</a></div> <div><a href="#block-structure">Block structure</a></div> <ul> <li><a href="#what-happens-within-block-structures">What happens within block structures?</a></li> <li><a href="#are-block-structures-useful">Are block structures useful?</a></li> <li><a href="#collapsing-the-block-structure">Collapsing the block structure</a></li> </ul> <div><a href="#cross-model-dynamics">Cross model dynamics</a></div> <div><a href="#effect-of-depth-and-width-on-the-models-outputs">Effect of depth and width on the models outputs</a></div> <div><a href="#discussion">Discussion</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>When applying artificial neural networks, performance can be optimized by varying the architecture depth and width. However, there is a lack of understanding of what effect scaling the models’ main parameters - depth and width - has on the learned representations. Do deep models learn different hidden layer features than wide models? Also, are there systematical differences in the outputs of deep and wide models?</p> <p>This lack of insight is tackled in the paper</p> <p></p> <p><span>   ▶  </span>Thao Nguyen et al. (ICLR, 2021) Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite>.</p> <p></p> <p>First, representation similarity is analyzed between different layers of a single model. It is found that in overparameterized models, so-called <em>block structures</em> arise, which refer to groups of contiguous layers which have very similar hidden representations. These block structures emerge independent of whether a model’s width or depth is increased.<br> Furthermore, it is shown that key components of the representations are preserved and propagated during block structure layers. This opens various questions:</p> <ul> <li>Are representations becoming more meaningful for the task at hand when being propagated through the block structure?</li> <li>Could block structure layers be pruned from the model without negatively affecting performance?</li> <li>What role do the residual connections, present in the models used in the experiments, play for keeping representations similar during propagation through the block structure?</li> </ul> <p>Those questions are addressed by Nguyen et al. <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite>, and their answers will be discussed and interactively visualized in the the following blogpost.</p> <p>In addition, the blogpost presents novel findings described by the second part of Nguyen et al. <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite>’s analysis, which deals with representation similarities between different models, as well as whether models of different architecture type (e.g. deep vs. wide) show systematical differences in output predictions, despite performing very similar overall.</p> <h2 id="centered-kernel-alignment">Centered kernel alignment</h2> <p>Let’s begin with having a look at methodology. It is not that easy to directly compare different layers representations, for example because of their varying size and their distributed nature (meaning that important features can rely on different neurons outputs). The tool Nguyen et al. <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite> use for measuring representation similarity, is called centered kernel alignment (CKA) <d-footnote>Note that Nguyen et al. <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite> propose a slightly modified version of CKA in their paper, namely minibatch CKA, which is used in their experiments. They modify classic CKA to reduce memory consumption, and their minibatch CKA converges to the same value as CKA when the whole dataset is considered a minibatch.</d-footnote>, and addresses those problems.<br> It follows a straight forward idea, which originates in neuroscience: Similarity is not measured between representations directly, but between representation (dis-)similarity matrices</p> <d-cite key="kriegeskorte2008representational"></d-cite> <p>.</p> <style>#floated{float:left}</style> <p>When $\mathbf{X}$ is a matrix, holding the encoded features of size $m \times p_1$ ($m$ being the number of examples, $p_1$ being the number of neurons), then $\mathbf{K}=\mathbf{XX}^\top$ refers to the $m \times m$ <ins>representation similarity matrix</ins> of our first comparison layer. We can now compare how similar $\mathbf{K}$ is to a second representation similarity matrix $\mathbf{L} = \mathbf{Y}\mathbf{Y}^\top$, with $\mathbf{Y}$ of size $m \times p_2$ holding the examples encoded after our second comparison layer.<br> In words, we do not compare representations, but how similar the relations between each layers representations are.</p> <p>Proceeding to CKA, not a lot of logic is added. First, both representation similarity matrices have to be centered (column and row means are being subtracted). This is done by computing $\mathbf{K}’ = \mathbf{HKH}$, with $\mathbf{H}$ being the centering matrix $\mathbf{H} = \mathbf{I}_n - \frac{1}{n} \boldsymbol{1}\boldsymbol{1}^\top$. In this case, $n$ equals $m$, and $\mathbf{L}’$ is computed accordingly. The similarity between $\mathbf{K}’$ and $\mathbf{L}’$ is then calculated using the Hilbert-Schmidt Independence Criterion (HSIC). More specifically, $\textrm{HSIC} (\mathbf{K}, \mathbf{L})= \textrm{vec}(\mathbf{K}’)\textrm{vec}(\mathbf{L}’)/(m-1)^2$, which is a measure invariant to orthogonal transformations, and thus to permutations of neurons <d-cite key="DBLP:conf/icml/Kornblith0LH19"></d-cite>. Finally, CKA normalizes $\textrm{HSIC}$ to make the similarity score invariant to isotropic scaling, and to get a nice range of values between 0 and 1.</p> <p>Finally, the CKA formula is:</p> \[\begin{equation} \textrm{CKA}(\mathbf{K}, \mathbf{L}) = \frac{\textrm{HSIC} (\mathbf{K'}, \mathbf{L'})}{\sqrt{\textrm{HSIC} (\mathbf{K}, \mathbf{K}) \textrm{HSIC} (\mathbf{L}, \mathbf{L})}}. \end{equation}\] <h2 id="block-structure">Block structure</h2> <p>The first part of the paper deals with representation structure within models when scaling their with and depth. Nguyen et al. <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite> train different variants of ResNets <d-cite key="DBLP:conf/cvpr/HeZRS16"></d-cite>, mostly on CIFAR-10, and calculate the CKA score for each pair of the models layers. Those are then visualized in heatmaps as seen below. Three notes on this:</p> <ul> <li>The models compared within a graphic perform very similarly, with the maximum test accuracy margin between two models being 1,9%.</li> <li>The number of layers indicated in the plot might be bigger than the number of layers indicated in the models name. This is due to the fact, that in the plots it is accounted not only for the convolutional layers, but for all intermediate representations. Additional representations e.g. arise through pooling layers.</li> <li>The chessboard like structure arises as a result of the residual connections: Representations after a residual connection are more similar to other post-residual representations, than to representations within a residual block.</li> </ul> <p>The first major finding in the paper, is that with increased width or depth of models, blocks of contiguous layers with a very high representation similarity arise. With the capacity of the models increasing, the blocks tend to get larger and more distinct. One can see the phenomenon emerge in the graphic below:</p> <div class="l-page"> <iframe src="/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations/assets/html/2022-12-01-how-scaling-a-neural-networks-width-or-depth-affect-its-hidden-representations/slider1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <p align="center" style="margin-top:20px"> <em> Figure 1: The block structures emerge when increasing the models depth (left) or width (right). </em> </p> <p>A block of contiguous layers, with CKA scores close to one, is termed <em>block structure</em>. It is visible that block structures arise independently of whether the models with or depth are increased.</p> <p>Next, it is investigated whether the block structures emerge with regard to absolute model size, or model size relative to the data. Nguyen et al. <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite> proceed by training models, with gradually reduced training data, while keeping the other model parameters static. The results can be explored below:</p> <div class="l-page"> <iframe src="/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations/assets/html/2022-12-01-how-scaling-a-neural-networks-width-or-depth-affect-its-hidden-representations/fig_2.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <p align="center" style="margin-top:20px"> <em> Figure 2: Reducing the training data leads to block structures emerging in models with less capacity. </em> </p> <p>It can be seen, that when reducing the data step by step, the blocks structures emerge in smaller models already, for both deep and wide models. Therefore, it is concluded, that the emergence of the block structure seems to be an artifact of overparameterized models.</p> <h3 id="what-happens-within-block-structures">What happens within block structures?</h3> <p>After gaining knowledge about the block structure, the consequent follow-up question is, what happens to the representations within the block structure. Revisiting how the CKA score was computed, note the two-step procedure that was used: First, representation similarity matrices were computed (for each layer in this case), which were then compared with other representation similarity matrices. This means that representations can numerically change between layers, however, the CKA score between those layers remains high in case the relative representation structure remains similar.</p> <p>So, what computations are done by the neural networks during block structure layers? For investigating, Nguyen et al. <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite> look at a rewritten version of the CKA score. When $\mathbf{X}$ and $\mathbf{Y}$, of sizes $n \times p_1$ and $n \times p_2$, hold the centered layer representations, the CKA score can be computed using the following formular:</p> \[\begin{equation} \textrm{CKA}(\mathbf{XX}^\top, \mathbf{YY}^\top) = \frac{\sum_{i=1}^{p_1} \sum_{j=1}^{p_2} \lambda_{X}^i \lambda_{Y}^j \langle \mathbf{u}_{X}^i\,,\mathbf{u}_{Y}^j\rangle^2} {\sqrt{\sum_{i=1}^{p_1} (\lambda_{X}^i)^2} \sqrt{\sum_{j=1}^{p_2} (\lambda_{Y}^j)^2}}. \end{equation}\] <p>The equation arises by rewriting $\mathbf{X}$ and $\mathbf{Y}$ in terms of their in terms of their singular value decompositions</p> <d-cite key="DBLP:conf/icml/Kornblith0LH19"></d-cite> <p>. The vectors \(\begin{equation}\mathbf{u}_{X}^i\end{equation}\) and \(\begin{equation} \mathbf{u}_{Y}^j \end{equation}\) refer to the \(\begin{equation} i^{\textrm{th}} \end{equation}\)/\(\begin{equation} j^{\textrm{th}}\end{equation}\) left-singular vector of \(\begin{equation} \mathbf{XX}^\top \end{equation}\)/\(\begin{equation} \mathbf{YY}^\top \end{equation}\), which refer to the normalized principle components of $\mathbf{X}$ and $\mathbf{Y}$ <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite>. Finally, $\lambda_{X}^i$ and $\lambda_{Y}^j$ are the corresponding squared singular values, which measure the fraction of variance explained by each principal component in the representations.</p> <p>Nguyen et al. <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite> find, that the fraction of variance, that is explained by the first principal component, is very high in network layers within a block structure, but low for other layers.</p> <p>If for two layers, all the variance in the representations would be explained by their first principal components, the CKA score between those layers would collapse to the squared alignment \(\begin{equation} \langle \mathbf{u}_{X}^i\,,\mathbf{u}_{Y}^j\rangle^2\end{equation}\) between those first principal components. It is thus suggested, that within block structure layers, where the CKA score is continuously close to 1, the first principal component is <em>preserved</em> and <em>propagated</em>.</p> <p>This theoretical finding is supported by analysing previously shown models with regard to the first principal component:</p> <div class="l-page"> <iframe src="/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations/assets/html/2022-12-01-how-scaling-a-neural-networks-width-or-depth-affect-its-hidden-representations/fig_3.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <p align="center" style="margin-top:20px"> <em> Figure 3: Four plots, for four different models: A deep model with block structure (left), a deep model without block structure (left, after clicking the 'No blocks' button), a wide model with block structure (right) and a wide model without block structure (right, after clicking the 'No blocks' button"). On the top right for each model, the CKA heatmap is plotted as usual. On the top left, the cosine similarity for each layers first principal component can be seen. The bottom left shows the variance explained in the representations by the first principal component of each layer. The bottom right shows the CKA heatmap for layers with the first principal component being deleted from the representation matrices. </em> </p> <p>In the bottom left, we can see the fraction of variance explained in the representations by the first principal component, per layer. Looking at the models exhibiting a block structure, it is very visible that the fraction of variance explained within the block structures is greatly larger than in non block structure layers. When swapping to models without a block structure, this difference becomes even more clear.</p> <p>The top-left plot shows the cosine similarity between the first principal components. We can see that for models exhibiting a block structure, the heat map looks very similar to the CKA score plotted top-right, which showcases that CKA reflects the alignment between the first principal components, if the fractions of variance explained by them approach 1. Looking at non block structure models, the CKA heatmap is visibly different from the heatmap comparing first principal components.</p> <p>Finally, the CKA heatmap for models with the first principal component removed from the representations is shown at the bottom right. For the overparameterized models, this removes the block structure from the heatmap. For the non block structure models, the heatmap remains mostly unchanged.</p> <p>“Together these results demonstrate that the block structure arises from preserving and propagating the first principal component across its constituent layers.” <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite>.</p> <p>Furthermore, all effects seem to be similar for the left and the right side, disregarding whether the models overparameterizations come from increased width or depth.</p> <h3 id="are-block-structures-useful">Are block structures useful?</h3> <p>While representations stay relatively similar in their relations to each other, when being propagated through the block structure, one could still ask whether transformations applied within block structures impact task performance.<br> For investigating this question, Nguyen et al. <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite> train linear probes after each layer of different models. A linear probe resembles a linear classifier, that maps directly from layers hidden representations to input examples labels <d-cite key="DBLP:conf/iclr/AlainB17"></d-cite>. Linear probe accuracies can be seen below, for two models with block structure, and two without. Note that two models of the same architecture type can exhibit different CKA heatmaps, due to their different initializations.</p> <div class="l-page"> <iframe src="/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations/assets/html/2022-12-01-how-scaling-a-neural-networks-width-or-depth-affect-its-hidden-representations/fig_4.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <p align="center" style="margin-top:20px"> <em> Figure 4: Top row: CKA heatmaps for models with different widths. Bottom row: Linear probe accuracy for each layer. The dashed green lines refer to the boundaries between ResNet stages. </em> </p> <p>For the models without block structures on the left, we can see the accuracy of the linear probes monotonically increasing, and with it the informativity of the hidden representations regarding the task at hand. For models exhibiting block structures on the right, the picture looks more complex: For layers within a block, and before a residual connection, the linear probe accuracy drops noticeably. For within block layers, after a residual connection, the accuracy increases only marginally.<br> Also, there seems to be a jump in linear probe accuracy at the post-residual connection before the block structure, and one in the very early layers, for both models on the right. It feels though, that in overparameterized, block structure models, more of the relevant logic is performed in individual layers. Other layers, especially during block structures, seem to mostly propagate information from previous layers. Furthermore, this propagation seems to heavily rely on the residual connections present in ResNets, as pre-residual layers within blocks seem to even worsen the representations.</p> <p>For researching the dynamics between block structure emergence and residual connections, Nguyen et al. <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite> trained plain convolutional neural networks, without residual connections with varying widths, and computed the corresponding CKA heatmaps:</p> <div class="l-page"> <iframe src="/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations/assets/html/2022-12-01-how-scaling-a-neural-networks-width-or-depth-affect-its-hidden-representations/fig_1b.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <p align="center" style="margin-top:20px"> <em> Figure 5: Block structures emerge in a model without residual connections with increased width. </em> </p> <p>As shown for different ResNets, one can see blocks arise with increased model capacity. Based on the above graphic, Nguyen et al. <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite> conclude that the emergence of block structures does not seem to be affected by the residual connections.<br> When carefully observing the pictures, however, there are differences to the block structures seen in the ResNets:</p> <ul> <li>The blocks are not as sharp at the borders, and also the CKA score within the blocks is not uniformly as high as in the block structures observed in overparameterized ResNets.</li> <li>When further increasing the model capacity, after block structures already emerged, blocks do not seem to change much anymore. Again, this is different to what was observed before in the ResNets, where the position and especially the size of the block structures kept changing.</li> </ul> <p>Finally, it still seems that the residual connections do play a role for the emergence, and especially nature of block structures. More on this can be read in our discussion.</p> <h3 id="collapsing-the-block-structure">Collapsing the block structure</h3> <p>We’ve seen that the block structure arises in overparameterized models, and also that it preserves and propagates key components of the representations. Also, we’ve seen that the amount of task relevant information in the representations barely rises during the block structure.<br> Another way of researching whether/how much block structure layers are contributing in solving the final task, is simply pruning block structure layers from the model, and see whether the final performance is affected. Nguyen et al. <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite> proceed to do exactly this, and for comparison, also non block structure layers are pruned from the models.<br> More precisely, ResNet blocks are deleted one-by-one, starting at the end of each ResNet stage. How this impacts the performance of models can be seen below, for two models that exhibit block structure, and two that do not:</p> <div class="l-page"> <iframe src="/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations/assets/html/2022-12-01-how-scaling-a-neural-networks-width-or-depth-affect-its-hidden-representations/fig_5.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <p align="center" style="margin-top:20px"> <em> Figure 6: Top row: CKA heatmaps for two narrow models (trained from different initializations) and two wide models. Bottom row: Test accuracy after deleting blocks from the end of each ResNet stage (indicated by the green dashed lines), while keeping the residual connections intact. The grey dashed line refers to the original models performance. </em> </p> <p>One can see that pruning blocks from the middle ResNet stage, and within a block structure, leads to only a small loss of overall performance. Furthermore, the size of the block structure seems to play a role: the performance loss in the second model from the right (large block structure) is even smaller than in the most right model (smaller block structure). When pruning residual blocks from other parts of the models, as well as in models without block structures, the performance loss is mostly pretty drastic.</p> <p>After two experiments researching the dynamics between block structure layers and final task performance, it forms the intuition that block structure layers seem to contribute little to the models overall performance. In more pronounced block structures, the block structure layers contribution to final performance seems to approach zero.</p> <h2 id="cross-model-dynamics">Cross model dynamics</h2> <p>Next, the similarity of representations between different models is investigated. Nguyen et al. <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite> compare how similar the representations of architecturally identical models, with different initializations, and how similar the representations of models with different architectures are. Again, the models compared exhibit a very similar testing accuracy.</p> <p>For models without block structures, the CKA heatmaps between layers of different models look very similar to the heatmaps between layers of the same model. This holds for when architecturally identical models are compared, as well as when architecturally different models are compared. In the latter case, the representations are still similar along the diagonal of the heatmap, suggesting similar representations at the same relative model depth.</p> <p>For models exhibiting block structures, there is some representation similarity for non block structure layers. However, when comparing layers of different models from which at least one layer is part of a block structure, in the within model heatmap, there is nearly no to no representation similarity at all. Representations during block structures therefore seem unique to each model.</p> <h2 id="effect-of-depth-and-width-on-the-models-outputs">Effect of depth and width on the models outputs</h2> <p>Finally, Nguyen et al. <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite> have investigated how model architectures and model outputs relate. They found that there are systematically different output predictions made by different model types, even though the average accuracy is very similar. The outputs are systematically different on class level, as well as on individual example level. Let’s have a look at the graphics below, to see those findings in detail:</p> <div class="l-page"> <iframe src="/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations/assets/html/2022-12-01-how-scaling-a-neural-networks-width-or-depth-affect-its-hidden-representations/fig_6.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe> </div> <p align="center" style="margin-top:20px"> <em> Figure 7: Left side: Comparison of example level accuracy of two groups of 100 networks each, trained on CIFAR-10. <strong>b</strong>: Two groups of ResNet-62 models compared for showcasing the variance in example level predictions that occurs by chance. <strong>a</strong>: A group of ResNet-14 (2x) compared with a group of ResNet-62s. One can see that the variance is much higher than in the left bottom plot. <strong>c</strong>: Comparison of testing accuracies for different groups of models trained on ImageNet, this time on class level. Orange dots: baseline comparison between ResNet-83 groups. Blue dots: Group of ResNet-83s and ResNet-50 (x2.8). </em> </p> <p>On the left bottom side of the plot (<strong>b</strong>), we can see the average accuracy for individual examples, of two groups of 100 deep ResNets, on the CIFAR-10 test set. Note that both groups have statistically indistinguishable average accuracies on the whole dataset. The graphic acts as a baseline, to show what amount of variance between two groups of models should be expected by chance. For comparison, on the top left (<strong>a</strong>), average individual example accuracies are shown for a group of wide models and a group of deep models. We can see that the individual example accuracies vary much more than in the top left plot, which indicates that wide and deep models make systematically different mistakes.</p> <p>Insights in systematic differences in model outputs on class level, are shown on the right side of the figure. This time, the class level accuracies of two groups of architectually identical deep models are compared as a baseline (orange dots), with ImageNet being the dataset. The blue dots compare a group of deep models and a group of wide models. Again, one can see that there are systematic differences in output predictions, this time on class level. Looking at individual classes, three out of the five classes where the group of wide models perform better, resemble scenes: seashore, library and book store. Following this intuition, Nguyen et al. <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite> find that the wide models significantly perform better on ImageNet classes that descend from “structure” or “geological formation”. Note that while the difference in accuracy is statistically significant, it is quite small ($74.9\% \pm 0.05 \ \mathrm{vs.} \ 74.6\% \pm 0.06$). Deep models on the other hand, performed significantly better on classes descending from “consumer goods”, again with a relatively small margin though ($72.4\% \pm 0.07 \ \mathrm{vs.} \ 72.1\% \pm 0.06$).</p> <p>In the reviewing process, Nguyen et al. <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite> hypothesize that wide layers might be better at capturing small details which would help when detecting scenes, while depth would help when global structure is important, which would help with consumer goods.</p> <h2 id="discussion">Discussion</h2> <p>Nguyen et al. <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite> have shown that a so called block structure arises in overparameterized models, no matter if they are deep or wide. A block structure consists of many contiguous neural network layers, whose representations show very high similarity. They found that within a block structure, the first principal component of the layers representation matrix is mainly propagated.<br> They proceed by relating block structure and task performance, and find that the task-informativity of representations barely rises during block structures. Also, it becomes visible that residual connections play an important role when propagating information through block structure layers.<br> Next, ResNet blocks were pruned from block structure models, and it was shown that block structure layers could be deleted from models with minimal performance loss during the middle stage of the ResNet.<br> When comparing different models representations, it was found that representations are similar regarding the relative depth of models with different architecture types. Representations within block structures, however, didn’t exhibit similarity with representations from layers of other models.<br> Finally, they look at the relation between model architectures and model outputs, and find that there are systematical different mistakes made by models of different architecture types. This is observed on individual example level and on class level, and even though the compared models performance is very similar on the whole dataset.</p> <p>A first point of discussion, is the dynamic between residual connections and the block structure. While Figure 5 shows that a block structure can also arise in a model without residual connections, there are differences in shape and delimitability. More specifically, the block structure arising in the CNN seems less clear, and also, it doesn’t increase size or clarity when increasing the model capacity further and further. Also, the linear probe experiment shown in Figure 4 showcases the importance of residual connections for keeping the representations task relevance throughout the block structure.<br> It remains a topic of research, thus, how well the findings about the nature of the block structure generalize to different architecture types. A hypothesis might be, that when unnecessarily increasing the model capacity of a ResNet, the flow of information relies more and more on the residual connections, which propagate previously acquired logic in the representations.<br> In contrast, the block structure layers of the CNN have to preserve the representations themselves, so e.g. a drop in task usefulness like seen for the pre-residual layers within a block structure in Figure 4, would probably not be observed. A continuous (but probalby small) rise in linear probe accuracy throughout the block structure CNNs seems more imaginable, and it would indicate a major difference in the nature of block structure layers in ResNets and non-residual models.</p> <p>Next, it remains the questions whether the block structure benefits overall models performance. When looking at Figure 4 again, we can see that the linear probe accuracy still slightly increases in layers within a block structure, after a residual connection. Furthermore, Nguyen et al. <d-cite key="DBLP:conf/iclr/NguyenRK21"></d-cite> present a table with each models test performance, and while accuracy seems to saturate at some capacity, large models still perform better than smaller models in nearly all cases, even though the large models exhibit massive block structures.<br> Also, it was shown in Figure 6, that pruning layers from the block structure in the middle stage reduces accuracy, however, only slightly. The accuracy seemed to decrease less, when the block structure was greater and more pronounced.<br> It seems though, that some useful transformations are still going on within block structures, however, the transformations turn less useful as the overparameterization increases.</p> <p>For practical purposes of the presented findings, neural architecture search/model design would be the field to benefit: Information about emerging block structures could be used to find a trade-off between efficient, and successful neural architecture search. Detecting a block structures indicates being in the overparameterized regime, meaning that training a less capacity model might yield a similar performance.</p> <p>Also, a direction of thought could be designing networks that prevent themselves from building blocks, e.g. through penalizing representation similarity in the loss. The idea would be, to still fight vanishing/exploding gradients using skip-connections</p> <d-cite key="DBLP:conf/cvpr/SzegedyLJSRAEVR15"></d-cite> <p>, while preventing the net from simply propagating information during block structures.<br> Whether this would yield a beneficial effect, or would solely take the self-regulating properties, brought by residual connections, from the model, remains a topic for future research.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/How-scaling-a-Neural-Networks-Width-or-Depth-affect-its-hidden-Representations/assets/bibliography/2022-12-01-how-scaling-a-neural-networks-width-or-depth-affect-its-hidden-representations.bib"></d-bibliography> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>